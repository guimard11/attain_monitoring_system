# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "5bb995e4-9bc1-42e4-b483-d62aadaafc35",
# META       "default_lakehouse_name": "LH_success_bronze",
# META       "default_lakehouse_workspace_id": "6a95caf4-a121-44cd-8a44-294a59cb0fc5",
# META       "known_lakehouses": [
# META         {
# META           "id": "5bb995e4-9bc1-42e4-b483-d62aadaafc35"
# META         },
# META         {
# META           "id": "ad6c2670-8e52-4cb1-a385-669f61c55b3f"
# META         }
# META       ]
# META     }
# META   }
# META }

# MARKDOWN ********************

# ## Cleaning raw data from 7 datasets stored in Lakehouse Bronze
# 
# Initial data processing is applied to the datasets downloaded and stored in the Bronze Lakehouse, which are then stored in the Silver Lakehouse for further processing. The main processing steps are summarized as follows:
# * Review the data type of variables.
# * Transform the data types of variables.
# * Create new variables from existing columns. For example, extract the day, month, quarter, and year from existing date columns.
# * Rename variables.
# * Rename the values of certain variables.

# MARKDOWN ********************

# ##### Loading the module required to process raw data in the datasets.

# CELL ********************

# Load required module from Pyspark
import requests
import pandas as pd
from datetime import timedelta
from pyspark.sql.functions import when, col, isnan, isnull,dayofmonth, month, quarter, year, round, to_date

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ##### Loading and reading the datasets into delta format 

# MARKDOWN ********************

# ###### 1. success_spark loading and reading
# 
# The success_spark dataset contains data on the Micro, Small and medium enterprises.

# CELL ********************

# Path to success_spark table in Bronze Lakehouse
bronze_success_spark_path = "abfss://success_pipeline@onelake.dfs.fabric.microsoft.com/LH_success_bronze.Lakehouse/Tables/dbo/success_spark"

# Load the success_client table into a DataFrame
success_spark = spark.read.format("delta").load(bronze_success_spark_path)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 2. success_job loading and reading
# 
# The success_job dataset contains data on the number of jobs created and the number of layoffs during the project implementation phase. 

# CELL ********************

# Path to success_job table in Bronze Lakehouse
bronze_success_job_path = "abfss://success_pipeline@onelake.dfs.fabric.microsoft.com/LH_success_bronze.Lakehouse/Tables/dbo/success_job"

# Load the sucess_job table into a Dataframe
success_job = spark.read.format("delta").load(bronze_success_job_path)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 3. success_sales loading and reading
# 
# The success_sales dataset contains data on the sales generated by the enterprise during the project implementation phase

# CELL ********************

# Path to success_sales table in Bronze Lakehouse
bronze_success_sales_path = "abfss://success_pipeline@onelake.dfs.fabric.microsoft.com/LH_success_bronze.Lakehouse/Tables/dbo/success_sales"

# Load the success_sales table into a Dataframe
success_sales = spark.read.format("delta").load(bronze_success_sales_path)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 4. success_participant loading and reading
# 
# The success_participant dataset contains data on the number of attendants to each activity support (Training, Coaching) implemented by implementing project partners to make the MSME more profitable.

# CELL ********************

# Path to success_participant_list table in Bronze Lakehouse
bronze_success_participant_path = "abfss://success_pipeline@onelake.dfs.fabric.microsoft.com/LH_success_bronze.Lakehouse/Tables/dbo/success_participant_list"

# Load the success_participant_list table into a Dataframe
success_participant = spark.read.format("delta").load(bronze_success_participant_path)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 5. success_hired_employee loading and reading
# 
# The success_hired_employee contains data on employee hiring cases.

# CELL ********************

# Path to success_hired_employee table in Bronze Lakehouse
bronze_success_hired_employee_path = "abfss://success_pipeline@onelake.dfs.fabric.microsoft.com/LH_success_bronze.Lakehouse/Tables/dbo/success_hired_employee"

# Load the success_hired_employee table into a Dataframe
success_hired = spark.read.format("delta").load(bronze_success_hired_employee_path)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 6. success_fired_employee loading and reading
# 
# The success_fired_employee contains data on employee firing or layoff cases.

# CELL ********************

# Path to success_fired_employee table in Bronze Lakehouse
bronze_success_fired_employee_path = "abfss://success_pipeline@onelake.dfs.fabric.microsoft.com/LH_success_bronze.Lakehouse/Tables/dbo/success_fired_employee"

# Load the success_fired_employee table into a Dataframe
success_fired = spark.read.format("delta").load(bronze_success_fired_employee_path)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 7. success_activity loading and reading
# 
# The success_activity dataset contains data on activity type implemented by implementing project partners.

# CELL ********************

# Path to success_activity_monitoring table in Bronze Lakehouse
bronze_success_activity_path = "abfss://success_pipeline@onelake.dfs.fabric.microsoft.com/LH_success_bronze.Lakehouse/Tables/dbo/success_activity_monitoring"

# Load the success_fired_employee table into a Dataframe
success_activity = spark.read.format("delta").load(bronze_success_activity_path)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### Removing the Null values from the attendance_number.
# 
# Every activity support in this project had at least 1 participant. Null value may be interpreted as an entry error. These lines can be removed from the dataset.

# CELL ********************

#Reorganize the structure of success_activity dataset by filtering out all null value in attendance_number column.
success_activity2=success_activity.filter(col("attendance_number").isNotNull())

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ##### Cleaning and process stage

# MARKDOWN ********************

# ###### 1. Clean and process the dataset success_spark

# CELL ********************

# Replace data value name into the column partenaire
success_updated = success_spark.replace({
    "ACME 2": "ACME",
    "ACME 3": "ACME",
    "AIDE 2": "AIDE",
    "CAPOSUD 2": "CAPOSUD",
    "CAPUC 2" : "CAPUC",
    "CEDEL 2": "CEDEL",
}, subset=["partenaire"])

# Replace data value name into the column "institution_financiere"
success_updated = success_updated.replace({
    "Oui": "yes",
    "Non": "no"
}, subset=["institution_financiere"])

# Replace data value name into the column "msme_productive"
success_updated = success_updated.replace({
    "Oui": "yes",
    "Non": "no"
}, subset=["msme_productive"])

# Replace data value name into the column "group_epargne"
success_updated = success_updated.replace({
    "Aucun": "No group",
    "Association": "Association",
    "AVEC": "AVEC group",
    "BD&SME": "BD&SME",
    "MUSO": "MUSO",
    "Solidarite": "Solidarity group"
}, subset=["group_epargne"])

# Replace all values inferior to 10000 and blank value in revenu_annuel
success_updated = success_updated.withColumn("revenu_annuel", when(col("revenu_annuel") < 25000, 0)\
                             .when(col("revenu_annuel").isNull(), 0)\
                             .otherwise(col("revenu_annuel")))\
                             .withColumn("nombre_employe", when(col("nombre_employe").isNull(), 1)\
                             .otherwise(col("nombre_employe")))\
                             .withColumn("nombre_employe", round(col("nombre_employe").cast("int")))\
                             .withColumn("age", when(col("age").isNull(), 18)\
                             .when((col("age")< 18) & (col("age")> 1), 18).otherwise(col("age")))\
                             .withColumn("secteur_activite", when(col("secteur_activite") == "Service", "Services")\
                              .when(col("secteur_activite").isNull(), "Services")\
                              .otherwise(col("secteur_activite")))\
                              .withColumn("date_enregistrement", to_date(col("date_enregistrement"), "M/d/yyyy"))\
                              .withColumn("day", dayofmonth(col("date_enregistrement")))\
                              .withColumn("month", month(col("date_enregistrement")))\
                              .withColumn("quarter", quarter(col("date_enregistrement")))\
                              .withColumn("year", year(col("date_enregistrement")))\
                              .withColumnRenamed("partenaire", "business_partner")\
                              .withColumnRenamed("institution_financiere", "financial_institution")\
                              .withColumnRenamed("date_enregistrement", "registration_date")\
                              .withColumnRenamed("nom_msme", "msme_name")\
                              .withColumnRenamed("revenu_annuel", "annual_income")\
                              .withColumnRenamed("type_msme", "msme_category")\
                              .withColumnRenamed("secteur_activite", "industry")\
                              .withColumnRenamed("nombre_employe", "employee_number")\
                              .withColumnRenamed("group_epargne", "savings_group")\
                              .withColumnRenamed("montant_pret", "loan_amount")\
                              .withColumnRenamed("age", "business_owner_age")\
                              .withColumnRenamed("sex", "business_owner_sex")
                              

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 2. clean and process the dataset success_job

# CELL ********************

# Rename the dataset to capture all process operations in a new one.

job_updated = success_job.withColumn("nbre_employe_embauche", when(col("nbre_employe_embauche").isNull(), 0)\
                         .otherwise(col("nbre_employe_embauche")))\
                         .withColumn("nombre_employe_licencie", when(col("nombre_employe_licencie").isNull(), 0)\
                         .otherwise(col("nombre_employe_licencie")))\
                         .withColumn("date_collecte", to_date(col("date_collecte"), "M/d/yyyy"))\
                         .withColumn("day", dayofmonth(col("date_collecte")))\
                         .withColumn("month", month(col("date_collecte")))\
                         .withColumn("quarter", quarter(col("date_collecte")))\
                         .withColumn("year", year(col("date_collecte")))\
                         .withColumnRenamed("nbre_employe_embauche", "total_hired_employee")\
                         .withColumnRenamed("nombre_employe_licencie", "total_fired_employee")\
                         .withColumnRenamed("date_collecte", "collecting_date")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 3. clean and process the dataset success_sales

# CELL ********************

# Replace null values, create new columns from existing the column date.

sales_updated = success_sales.withColumn("domestic_sale", when(col("domestic_sale").isNull(), 0)\
                         .otherwise(col("domestic_sale")))\
                         .withColumn("sales_reporting_date", to_date(col("sales_reporting_date"), "M/d/yyyy"))\
                         .withColumn("day_sales_reporting", dayofmonth(col("sales_reporting_date")))\
                         .withColumn("month_sales_reporting", month(col("sales_reporting_date")))\
                         .withColumn("quarter_sales_reporting", quarter(col("sales_reporting_date")))\
                         .withColumn("year_sales_reporting", year(col("sales_reporting_date")))\
                         .withColumn("international_sale", when(col("international_sale").isNull(), 0)\
                         .otherwise(col("international_sale")))
            

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 4. Clean and process the dataset success_participant_list

# CELL ********************

# Rename the activity_key column to keep the same in other database.
participant_updated = success_participant.withColumnRenamed("activity_key", "activity_id")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 5. Clean and process the dataset success_hired_employee.

# CELL ********************

#Replace the Oui/Non values by yes/no values in 4 variables using mapping and replace functions.
cols = [
    "contrat_temps_plein",
    "est_assure",
    "grace_support_meda",
    "est_decent_work"
]

mapping = {
    "Oui": "yes",
    "Non": "no",
    "oui": "yes",
    "non": "no"
}

hiring_updated = success_hired.replace(mapping, subset=cols)

#Rename columns and round numeric values to integer the duree_contrat variable
hiring_updated = hiring_updated.withColumn("duree_contrat", round(col("duree_contrat").cast("int")))\
                               .withColumnRenamed("sexe", "sex")\
                               .withColumnRenamed("contrat_temps_plein", "is_full_time")\
                               .withColumnRenamed("est_assure", "have_insurance")\
                               .withColumnRenamed("grace_support_meda", "thanks_to_project_support")\
                               .withColumnRenamed("est_decent_work", "is_decent_work")\
                               .withColumnRenamed("duree_contrat", "contract_duration")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 6. Clean and process the dataset success_fired_employee

# CELL ********************

# Rename the variable sexe and transform certain age values based on assumptions and knowledge on the data collecting step.
firing_updated = success_fired.withColumn("age", when(col("age") == 1518, 17)\
                              .when(col("age")== 1935, 27)\
                              .when(col("age")== 3665, 51)\
                              .otherwise(col("age")))\
                              .withColumnRenamed("sexe", "sex")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 7. Clean and process the dataset success_activity

# CELL ********************

# Rename the activity_key, then extract day, motnh, quarter and year from column date and integer the activity_duration variable.
activity_updated = success_activity2.withColumn("activity_duration", round(col("activity_duration").cast("int")))\
                                    .withColumn("activity_date", to_date(col("activity_date"), "M/d/yyyy"))\
                                    .withColumn("day_activity", dayofmonth(col("activity_date")))\
                                    .withColumn("month_activity", month(col("activity_date")))\
                                    .withColumn("quarter_activity", quarter(col("activity_date")))\
                                    .withColumn("year_activity", year(col("activity_date")))\
                                    .withColumnRenamed("activity_key", "activity_id")


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ##### Saving and writing stage

# MARKDOWN ********************

# ###### 1. Save the dataset success_spark into the Lakehouse Silver (LH_success_silver) as success_updated.

# CELL ********************

## Save the success_updated dataset into the Lakehouse Silver
success_updated_path = "abfss://success_pipe_updated@onelake.dfs.fabric.microsoft.com/LH_success_silver.Lakehouse/Tables/dbo/success_updated"
success_updated.write.format("delta").mode("overwrite").save(success_updated_path )

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark",
# META   "frozen": false,
# META   "editable": true
# META }

# MARKDOWN ********************

# ###### 2. Save the dataset success_job into the Lakehouse Silver (LH_success_silver) as job_updated.

# CELL ********************

## Save the job_updated dataset into the Lakehouse Silver
job_updated_path = "abfss://success_pipe_updated@onelake.dfs.fabric.microsoft.com/LH_success_silver.Lakehouse/Tables/dbo/job_updated"
job_updated.write.format("delta").mode("overwrite").save(job_updated_path )

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# 3. Save the dataset success_sales into the Lakehouse Silver (LH_success_silver) as sales_updated.

# CELL ********************

## Save the sales_updated dataset into the Lakehouse Silver
sales_updated_path = "abfss://success_pipe_updated@onelake.dfs.fabric.microsoft.com/LH_success_silver.Lakehouse/Tables/dbo/sales_updated"
sales_updated.write.format("delta").mode("overwrite").save(sales_updated_path )

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 4. Save the dataset success_participants_list into the Lakehouse Silver (LH_success_silver) as participants_updated.

# CELL ********************

## Save the participant_updated dataset into the Lakehouse Silver
participant_updated_path = "abfss://success_pipe_updated@onelake.dfs.fabric.microsoft.com/LH_success_silver.Lakehouse/Tables/dbo/participant_updated"
participant_updated.write.format("delta").mode("overwrite").save(participant_updated_path )

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 5. Save the dataset success_hired into the Lakehouse Silver (LH_success_silver) as hiring_updated.

# CELL ********************

## Save the hiring_updated dataset into the Lakehouse Silver
hiring_updated_path = "abfss://success_pipe_updated@onelake.dfs.fabric.microsoft.com/LH_success_silver.Lakehouse/Tables/dbo/hiring_updated"
hiring_updated.write.format("delta").mode("overwrite").save(hiring_updated_path )

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 6. Save the dataset success_fired into the Lakehouse Silver (LH_success_silver) as firing_updated.

# CELL ********************

## Save the firing_updated dataset into the Lakehouse Silver
firing_updated_path = "abfss://success_pipe_updated@onelake.dfs.fabric.microsoft.com/LH_success_silver.Lakehouse/Tables/dbo/firing_updated"
firing_updated.write.format("delta").mode("overwrite").save(firing_updated_path )

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###### 7. Save the dataset success_activity into the Lakehouse Silver (LH_success_silver) as activity_updated.

# CELL ********************

## Save the activity_updated dataset into the Lakehouse Silver
activity_updated_path = "abfss://success_pipe_updated@onelake.dfs.fabric.microsoft.com/LH_success_silver.Lakehouse/Tables/dbo/activity_updated"
activity_updated.write.format("delta").mode("overwrite").save(activity_updated_path )

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
